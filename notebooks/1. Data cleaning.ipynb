{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_9nryk7DPtX"
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBARle-BiX4b"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9251,
     "status": "ok",
     "timestamp": 1746248789560,
     "user": {
      "displayName": "Francisco",
      "userId": "11190877053062531761"
     },
     "user_tz": 300
    },
    "id": "UDa2w5NgiZKW",
    "outputId": "25ecbb6e-1df9-4b79-9cd2-43a576f50a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastexcel in /Users/ulisesgordillo/anaconda3/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ulisesgordillo/anaconda3/lib/python3.11/site-packages (from fastexcel) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/ulisesgordillo/anaconda3/lib/python3.11/site-packages (from pyarrow>=8.0.0->fastexcel) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastexcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1722,
     "status": "ok",
     "timestamp": 1746248791290,
     "user": {
      "displayName": "Francisco",
      "userId": "11190877053062531761"
     },
     "user_tz": 300
    },
    "id": "Z_JWQ-EZFOd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ulisesgordillo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l751Y4pzFSGQ"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746248814499,
     "user": {
      "displayName": "Francisco",
      "userId": "11190877053062531761"
     },
     "user_tz": 300
    },
    "id": "2r_OfAHoFWCq"
   },
   "outputs": [],
   "source": [
    "PATH_DENSITY_REPORT       = 'DensityReports.xlsx'\n",
    "PATH_HISTORICAL_INCIDENTS = 'HistoricalIncidents.xlsx'\n",
    "PATH_PRODUCT_ATTRIBUTES   = 'ProductAttributes.xlsx'\n",
    "PATH_SUPPLIER_SCORECARD   = 'SupplierScorecard.xlsx'\n",
    "\n",
    "EXPORT_DENSITY_REPORT       = 'density_report.csv'\n",
    "EXPORT_HISTORICAL_INCIDENTS = 'historical_incidents.csv'\n",
    "EXPORT_PRODUCT_ATTRIBUTES   = 'product_attributes.csv'\n",
    "EXPORT_SUPPLIER_SCORECARD   = 'supplier_scorecard.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZpkTTkwilQu"
   },
   "source": [
    "## Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1746248833876,
     "user": {
      "displayName": "Francisco",
      "userId": "11190877053062531761"
     },
     "user_tz": 300
    },
    "id": "zut0lQNJioXd"
   },
   "outputs": [],
   "source": [
    "def polars_read_excel(file_name, sheet_name='Sheet1'):\n",
    "  return pl.read_excel(source=file_name, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to read the excel files\n",
    "df_density_report = polars_read_excel(PATH_DENSITY_REPORT)\n",
    "df_historical_incidents = polars_read_excel(PATH_HISTORICAL_INCIDENTS)\n",
    "df_product_attributes = polars_read_excel(PATH_PRODUCT_ATTRIBUTES)\n",
    "df_supplier_scorecard = polars_read_excel(PATH_SUPPLIER_SCORECARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sngLbxheFs0s"
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h23cwY4AFwlb"
   },
   "source": [
    "## 1. Density Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Product Reference` issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_reference(series: pl.Series) -> pl.Series:\n",
    "    \"\"\"\n",
    "    Cleans ProductReference: removes trailing 'X', validates 'PRD'+5 digits format.\n",
    "    Returns cleaned string if valid, otherwise None (null).\n",
    "    \"\"\"\n",
    "    series_str = series.cast(pl.Utf8, strict=False)\n",
    "    cleaned_series = series_str.str.strip_chars_end('X')\n",
    "    valid_pattern = r\"^PRD\\d{5}$\"\n",
    "    final_series = pl.when(cleaned_series.str.contains(valid_pattern)) \\\n",
    "                     .then(cleaned_series) \\\n",
    "                     .otherwise(None)\n",
    "    return final_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null count in df_density_report['ProductReference']: 7652\n",
      "Null count in df_historical_incidents['ProductReference']: 301\n"
     ]
    }
   ],
   "source": [
    "df_density_report = df_density_report.with_columns(\n",
    "    clean_product_reference(pl.col(\"ProductReference\")).alias(\"ProductReference\")\n",
    ")\n",
    "# Calculate and print null count\n",
    "null_count_dr = df_density_report['ProductReference'].is_null().sum()\n",
    "print(f\"Null count in df_density_report['ProductReference']: {null_count_dr}\")\n",
    "\n",
    "\n",
    "# Apply cleaning to df_historical_incidents\n",
    "df_historical_incidents = df_historical_incidents.with_columns(\n",
    "    clean_product_reference(pl.col(\"ProductReference\")).alias(\"ProductReference\")\n",
    ")\n",
    "# Calculate and print null count\n",
    "null_count_hi = df_historical_incidents['ProductReference'].is_null().sum()\n",
    "print(f\"Null count in df_historical_incidents['ProductReference']: {null_count_hi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4fKCXnSh8N-"
   },
   "source": [
    "### Naming consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7x882Zdh8N_"
   },
   "source": [
    "We will check if there are  grammar errors in columns like `SupplierName`, `GarmentType`, `Material`, `ProposedFoldingMethod`, `ProposedLayout` and `PackagingQuality`to avoid repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uecfqqtph8OA",
    "outputId": "69b9fef3-bdad-4dc4-d0f2-72f0e94304fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Unique Values for Potential Inconsistencies\n",
      "Unique values in: SupplierName\n",
      "['SPLF', 'SuplA', 'SupllierC', 'SuppB', 'SupplierA', 'SupplierB', 'SupplierC', 'SupplierD', 'SupplierE', 'SupplierF', 'SupplierG', 'SupplierH', 'supplierA', 'supplierh']\n",
      "Total unique non-null values: 14\n",
      "Unique values in: GarmentType\n",
      "['Blouse', 'Coat', 'Dress', 'Hoodie', 'Jacket', 'Pants', 'Shirt', 'Shorts', 'Skirt', 'Suit', 'Sweater', 'T-Shirt']\n",
      "Total unique non-null values: 12\n",
      "Unique values in: Material\n",
      "['Cotton', 'Denim', 'Linen', 'Polyester', 'Silk', 'Wool']\n",
      "Total unique non-null values: 6\n",
      "Unique values in: ProposedFoldingMethod\n",
      "['FoldX', 'Methd1', 'Method1', 'Method2', 'Method3', 'Method_2', 'None']\n",
      "Total unique non-null values: 7\n",
      "Unique values in: ProposedLayout\n",
      "['Box9', 'LayC', 'LayoutA', 'LayoutB', 'LayoutC', 'LayoutD', 'LayoutE', 'LayoutX', 'layouta']\n",
      "Total unique non-null values: 9\n",
      "Unique values in: PackagingQuality\n",
      "['Bad', 'GOOD', 'Good', 'Uncertain', 'bad']\n",
      "Total unique non-null values: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_to_check = [\n",
    "    'SupplierName',\n",
    "    'GarmentType',\n",
    "    'Material',\n",
    "    'ProposedFoldingMethod',\n",
    "    'ProposedLayout',\n",
    "    'PackagingQuality']\n",
    "\n",
    "print(\"Checking Unique Values for Potential Inconsistencies\")\n",
    "\n",
    "for col_name in columns_to_check:\n",
    "    if col_name in df_density_report.columns:\n",
    "        try:\n",
    "            unique_values = (\n",
    "                df_density_report[col_name]\n",
    "                .unique()\n",
    "                .sort()\n",
    "            )\n",
    "\n",
    "            print(f\"Unique values in: {col_name}\")\n",
    "            print(unique_values.to_list())\n",
    "            print(f\"Total unique non-null values: {len(unique_values.drop_nulls())}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Could not process column: {col_name} \")\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Column not found: {col_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjUU-DiKh8OA"
   },
   "source": [
    "As we can see, we do have some errors in naming in columns `SupplierName`, `ProposedFoldingMethod`, `ProposedLayout` and `PackagingQuality`. So we need to fix these inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to nsure that the `SupplierName` column is consistent across all relevant datasets (`df_density_report`, `df_historical_incidents`, `df_supplier_scorecard`). This involves correcting typos, standardizing capitalization and removing extra whitespace found during initial checks.\n",
    "\n",
    "Instead of repeating the cleaning code for each datframe, we define the cleaning steps once. We then loop through the affected dataframes and apply this standardized cleaning logic to each one that contains the `SupplierName` column!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wr0GO9LOh8OB",
    "outputId": "f3a07269-0fb4-4ebf-9bc8-f3e82cfe7c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unique values in: SupplierName\n",
      "['SupplierA', 'SupplierB', 'SupplierC', 'SupplierD', 'SupplierE', 'SupplierF', 'SupplierG', 'SupplierH']\n",
      "Total unique non-null values: 8\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the mapping dictionary \n",
    "supplier_mapping = {\n",
    "    'SuplA': 'SupplierA',\n",
    "    'supplierA': 'SupplierA',\n",
    "    'SuppB': 'SupplierB',\n",
    "    'SupllierC': 'SupplierC',\n",
    "    'SPLF': 'SupplierF',       \n",
    "    'supplierh': 'SupplierH',\n",
    "    \n",
    "}\n",
    "\n",
    "# 2. Define the cleaning expression \n",
    "def clean_supplier_name_expr():\n",
    "  \"\"\"Returns a Polars expression to clean the SupplierName column.\"\"\"\n",
    "  return (\n",
    "      pl.col('SupplierName')\n",
    "        .str.strip_chars()          \n",
    "        .replace(supplier_mapping) \n",
    "        .str.replace_all(\" \", \"\")    \n",
    "        .alias('SupplierName')       \n",
    "  )\n",
    "\n",
    "# 3. Apply the cleaning expression to the DataFrames\n",
    "dataframes_to_clean = [\n",
    "    df_density_report,\n",
    "    df_historical_incidents,\n",
    "    df_supplier_scorecard\n",
    "    ]\n",
    "\n",
    "dataframe_dict = {\n",
    "    \"df_density_report\": df_density_report,\n",
    "    \"df_historical_incidents\": df_historical_incidents,\n",
    "    \"df_supplier_scorecard\": df_supplier_scorecard\n",
    "}\n",
    "\n",
    "\n",
    "cleaned_dataframes = {} \n",
    "for i, df in enumerate(dataframes_to_clean): \n",
    "    df_name = f\"DataFrame at index {i}\" \n",
    "   \n",
    "\n",
    "    if 'SupplierName' in df.columns:\n",
    "        \n",
    "        \n",
    "        df_cleaned = df.with_columns(clean_supplier_name_expr())\n",
    "        dataframes_to_clean[i] = df_cleaned \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(f\" -> Skipping {df_name} (no 'SupplierName' column).\")\n",
    "\n",
    "\n",
    "df_density_report = dataframes_to_clean[0]\n",
    "df_historical_incidents = dataframes_to_clean[1]\n",
    "df_supplier_scorecard = dataframes_to_clean[2] \n",
    "\n",
    "# Check the unique values again\n",
    "unique_values = (\n",
    "    df_density_report['SupplierName']\n",
    "    .unique()\n",
    "    .sort()\n",
    ")\n",
    "print(f\" Unique values in: SupplierName\")\n",
    "print(unique_values.to_list())\n",
    "print(f\"Total unique non-null values: {len(unique_values.drop_nulls())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tGV7VCvbh8OB",
    "outputId": "16209a05-a3b9-4ac5-a2b4-a2ace9659fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unique values in: ProposedFoldingMethod\n",
      "['FoldX', 'Method1', 'Method2', 'Method3', 'None']\n",
      "Total unique non-null values: 5\n"
     ]
    }
   ],
   "source": [
    "#fixing ProposedFoldingMethod\n",
    "method_mapping = {\n",
    "    'Methd1': 'Method1',\n",
    "    'Method_2': 'Method2',\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df_density_report = df_density_report.with_columns(\n",
    "    pl.col('ProposedFoldingMethod')\n",
    "      .str.strip_chars()\n",
    "      .replace(method_mapping)\n",
    "      .str.replace_all(\" \", \"\")\n",
    "      .alias('ProposedFoldingMethod')\n",
    ")\n",
    "\n",
    "# Check the unique values again\n",
    "unique_values = (\n",
    "    df_density_report['ProposedFoldingMethod']\n",
    "    .unique()\n",
    "    .sort()\n",
    ")\n",
    "print(f\" Unique values in: ProposedFoldingMethod\")\n",
    "print(unique_values.to_list())\n",
    "print(f\"Total unique non-null values: {len(unique_values.drop_nulls())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GVcHKOGMh8OD",
    "outputId": "cc499f3f-4c8a-435f-fb56-bd8f215dd388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in: ProposedLayout\n",
      "['Box9', 'LayoutA', 'LayoutB', 'LayoutC', 'LayoutD', 'LayoutE', 'LayoutX']\n",
      "Total unique non-null values: 7\n"
     ]
    }
   ],
   "source": [
    "#fixing ProposedLayout\n",
    "layout_mapping = {\n",
    "    'layouta': 'LayoutA',\n",
    "    'LayC': 'LayoutC',\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df_density_report = df_density_report.with_columns(\n",
    "    pl.col('ProposedLayout')\n",
    "      .str.strip_chars()\n",
    "      .replace(layout_mapping)\n",
    "      .str.replace_all(\" \", \"\")\n",
    "      .alias('ProposedLayout')\n",
    ")\n",
    "\n",
    "# Check the unique values again\n",
    "unique_values = (\n",
    "    df_density_report['ProposedLayout']\n",
    "    .unique()\n",
    "    .sort()\n",
    ")\n",
    "print(f\"Unique values in: ProposedLayout\")\n",
    "print(unique_values.to_list())\n",
    "print(f\"Total unique non-null values: {len(unique_values.drop_nulls())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qJ8JfXPjh8OF",
    "outputId": "1a4d01fd-e617-454d-cc1f-5dae375f3a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unique values in: PackagingQuality\n",
      "['Bad', 'Good', 'Uncertain']\n",
      "Total unique non-null values: 3\n"
     ]
    }
   ],
   "source": [
    "#fixing PackagingQuality\n",
    "quality_mapping = {\n",
    "    'GOOD': 'Good',\n",
    "    'bad': 'Bad',\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df_density_report = df_density_report.with_columns(\n",
    "    pl.col('PackagingQuality')\n",
    "      .str.strip_chars()\n",
    "      .replace(quality_mapping)\n",
    "      .str.replace_all(\" \", \"\")\n",
    "      .alias('PackagingQuality')\n",
    ")\n",
    "\n",
    "# Check the unique values again\n",
    "unique_values = (\n",
    "    df_density_report['PackagingQuality']\n",
    "    .unique()\n",
    "    .sort()\n",
    ")\n",
    "print(f\" Unique values in: PackagingQuality\")\n",
    "print(unique_values.to_list())\n",
    "print(f\"Total unique non-null values: {len(unique_values.drop_nulls())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5HSDPZ9h8OG"
   },
   "source": [
    "### `Folding Method` nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T1BNa2bah8OH",
    "outputId": "c396cf49-981a-4cd2-bf8b-68dcc5cfd22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folding Method Counts:\n",
      "shape: (5, 2)\n",
      "┌───────────────────────┬────────┐\n",
      "│ ProposedFoldingMethod ┆ count  │\n",
      "│ ---                   ┆ ---    │\n",
      "│ str                   ┆ u32    │\n",
      "╞═══════════════════════╪════════╡\n",
      "│ Method2               ┆ 220595 │\n",
      "│ Method1               ┆ 160102 │\n",
      "│ Method3               ┆ 114363 │\n",
      "│ None                  ┆ 2514   │\n",
      "│ FoldX                 ┆ 2426   │\n",
      "└───────────────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# unique values in folding method and the amount of times they appear\n",
    "folding_method_counts = (\n",
    "    df_density_report['ProposedFoldingMethod']\n",
    "    .value_counts()\n",
    "    .sort('count', descending=True)\n",
    ")\n",
    "print(f\"Folding Method Counts:\")\n",
    "print(folding_method_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the string 'None' with actual null (Python None)\n",
    "df_density_report = df_density_report.with_columns(\n",
    "pl.when(pl.col('ProposedFoldingMethod') == 'None')\n",
    ".then(None)\n",
    ".otherwise(pl.col('ProposedFoldingMethod'))\n",
    ".alias('ProposedFoldingMethod')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`None` is counted as a string, so we need to change it to `null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the string 'None' with actual null (Python None)\n",
    "df_density_report = df_density_report.with_columns(\n",
    "pl.when(pl.col('ProposedFoldingMethod') == 'None')\n",
    ".then(None)\n",
    ".otherwise(pl.col('ProposedFoldingMethod'))\n",
    ".alias('ProposedFoldingMethod')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5EvlhoLh8OI"
   },
   "source": [
    "Instead of just deleting these rows (and losing data) or picking a random method, we'll make an educated guess based on patterns in the existing data. The most logical assumption is that garments of a similar type, especially when intended for a specific packaging layout are likely folded using the same standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Enhanced mode calculated.\n",
      "- Combined mode calculated.\n",
      "- Garment mode calculated.\n",
      "- Global mode calculated: Method2\n",
      "\n",
      "Combining mode lookup tables...\n",
      "- Mode lookup tables combined.\n",
      "\n",
      "Joining combined lookup table to main DataFrame...\n",
      "- Join complete.\n",
      "\n",
      "Applying coalesce for imputation...\n",
      "- Imputation complete, temporary columns dropped.\n",
      "\n",
      "Null count in ProposedFoldingMethod AFTER imputation: 0\n",
      "Successfully imputed all original null values in ProposedFoldingMethod.\n",
      "Final value counts:\n",
      "shape: (4, 2)\n",
      "┌───────────────────────┬────────┐\n",
      "│ ProposedFoldingMethod ┆ count  │\n",
      "│ ---                   ┆ ---    │\n",
      "│ str                   ┆ u32    │\n",
      "╞═══════════════════════╪════════╡\n",
      "│ Method2               ┆ 222021 │\n",
      "│ Method1               ┆ 160668 │\n",
      "│ Method3               ┆ 114885 │\n",
      "│ FoldX                 ┆ 2426   │\n",
      "└───────────────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# 1a. Enhanced Mode (GarmentType, Layout, Material)\n",
    "mode_map_enhanced = (\n",
    "    df_density_report\n",
    "    .filter(\n",
    "        pl.col('ProposedFoldingMethod').is_not_null() &\n",
    "        pl.col('GarmentType').is_not_null() &\n",
    "        pl.col('ProposedLayout').is_not_null() &\n",
    "        pl.col('Material').is_not_null()\n",
    "        )\n",
    "    .group_by(['GarmentType', 'ProposedLayout', 'Material'])\n",
    "    .agg(pl.col('ProposedFoldingMethod').mode().first().alias('Mode_Enhanced'))\n",
    ")\n",
    "print(\"- Enhanced mode calculated.\")\n",
    "\n",
    "# 1b. Combined Mode (GarmentType, Layout)\n",
    "mode_map_combined = (\n",
    "    df_density_report\n",
    "    .filter(\n",
    "        pl.col('ProposedFoldingMethod').is_not_null() &\n",
    "        pl.col('GarmentType').is_not_null() &\n",
    "        pl.col('ProposedLayout').is_not_null()\n",
    "        )\n",
    "    .group_by(['GarmentType', 'ProposedLayout'])\n",
    "    .agg(pl.col('ProposedFoldingMethod').mode().first().alias('Mode_Combined'))\n",
    ")\n",
    "print(\"- Combined mode calculated.\")\n",
    "\n",
    "# 1c. Garment Mode (GarmentType only)\n",
    "mode_map_garment = (\n",
    "    df_density_report\n",
    "    .filter(pl.col('ProposedFoldingMethod').is_not_null())\n",
    "    .group_by('GarmentType')\n",
    "    .agg(pl.col('ProposedFoldingMethod').mode().first().alias('Mode_Garment'))\n",
    ")\n",
    "print(\"- Garment mode calculated.\")\n",
    "\n",
    "# 1d. Global Mode Fallback\n",
    "global_mode_folding = df_density_report.filter(\n",
    "    pl.col('ProposedFoldingMethod').is_not_null()\n",
    ")['ProposedFoldingMethod'].mode().first()\n",
    "print(f\"- Global mode calculated: {global_mode_folding}\")\n",
    "\n",
    "if global_mode_folding is None:\n",
    "    print(\"Warning: Global folding mode is None. Setting a default.\")\n",
    "    # Set a reasonable default if necessary (e.g., overall most frequent)\n",
    "    global_mode_folding = 'Method2' # Example default, adjust if needed\n",
    "\n",
    "# --- Step 2: Create a Combined Lookup Table ---\n",
    "print(\"\\nCombining mode lookup tables...\")\n",
    "\n",
    "# Start with the most specific mode map\n",
    "lookup_table = mode_map_enhanced\n",
    "\n",
    "# Left join the next level fallback mode\n",
    "lookup_table = lookup_table.join(\n",
    "    mode_map_combined, on=['GarmentType', 'ProposedLayout'], how='left'\n",
    ")\n",
    "\n",
    "# Left join the next level fallback mode\n",
    "lookup_table = lookup_table.join(\n",
    "    mode_map_garment, on=['GarmentType'], how='left'\n",
    ")\n",
    "print(\"- Mode lookup tables combined.\")\n",
    "# print(lookup_table.head()) # Optional: view combined lookup\n",
    "\n",
    "# --- Step 3: Perform ONE Join to the Main DataFrame ---\n",
    "print(\"\\nJoining combined lookup table to main DataFrame...\")\n",
    "df_with_modes = df_density_report.join(\n",
    "    lookup_table,\n",
    "    on=['GarmentType', 'ProposedLayout', 'Material'], # Join on the most specific keys\n",
    "    how='left' # Keep all rows from df_density_report\n",
    ")\n",
    "print(\"- Join complete.\")\n",
    "\n",
    "# --- Step 4: Apply Coalesce using Joined Columns ---\n",
    "print(\"\\nApplying coalesce for imputation...\")\n",
    "df_final = df_with_modes.with_columns(\n",
    "    pl.coalesce(\n",
    "        pl.col('ProposedFoldingMethod'),   # 1. Original value (keeps non-nulls)\n",
    "        pl.col('Mode_Enhanced'),           # 2. Mode from (Type, Layout, Material)\n",
    "        pl.col('Mode_Combined'),           # 3. Mode from (Type, Layout)\n",
    "        pl.col('Mode_Garment'),            # 4. Mode from (Type)\n",
    "        pl.lit(global_mode_folding)        # 5. Global Mode\n",
    "    ).alias('ProposedFoldingMethod') # Overwrite the original column directly\n",
    ")\n",
    "\n",
    "# --- Step 5: Drop temporary mode columns ---\n",
    "# Drop the columns brought in by the join\n",
    "df_final = df_final.drop(['Mode_Enhanced', 'Mode_Combined', 'Mode_Garment'])\n",
    "print(\"- Imputation complete, temporary columns dropped.\")\n",
    "\n",
    "\n",
    "# --- Verification ---\n",
    "final_null_count_folding = df_final['ProposedFoldingMethod'].is_null().sum()\n",
    "print(f\"\\nNull count in ProposedFoldingMethod AFTER imputation: {final_null_count_folding}\")\n",
    "\n",
    "if final_null_count_folding == 0:\n",
    "    print(\"Successfully imputed all original null values in ProposedFoldingMethod.\")\n",
    "    print(\"Final value counts:\")\n",
    "    print(df_final['ProposedFoldingMethod'].value_counts().sort(\"count\", descending=True))\n",
    "else:\n",
    "    print(f\"Warning: {final_null_count_folding} null values remain. Review mode calculations/fallbacks.\")\n",
    "\n",
    "# --- Assign back ---\n",
    "df_density_report = df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cx1aBvslh8OU"
   },
   "source": [
    "### Outliers for `ProposedUnitsPerCarton`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqVQiclCh8OU"
   },
   "source": [
    "We will check some strange values we found in:`ProposedUnitsPerCarton`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "F9lj11och8OW",
    "outputId": "3b013831-ad33-4b22-ca44-78bcf26633a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>statistic</th>\n",
       "      <td>count</td>\n",
       "      <td>null_count</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>min</td>\n",
       "      <td>25%</td>\n",
       "      <td>50%</td>\n",
       "      <td>75%</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProposedUnitsPerCarton</th>\n",
       "      <td>500000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.981055</td>\n",
       "      <td>864.741016</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0           1          2           3    4  \\\n",
       "statistic                  count  null_count       mean         std  min   \n",
       "ProposedUnitsPerCarton  500000.0         0.0  99.981055  864.741016 -3.0   \n",
       "\n",
       "                           5     6     7       8  \n",
       "statistic                25%   50%   75%     max  \n",
       "ProposedUnitsPerCarton  16.0  25.0  32.0  9999.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value distribution for ProposedUnitsPerCarton\n",
    "df_density_report.select(\n",
    "    pl.col('ProposedUnitsPerCarton')\n",
    ").describe().to_pandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eqa7Xc2Jh8O3"
   },
   "source": [
    "We can see three very noticeable things in `ProposedUnitsPerCarton`. First, we have negative values, which should be impossible we also have some decimal values different than 0, something that should be impossible and finally a maximum value of 9999. We need to clean this situation. These are probably sentinel values, values that are designed to fill the data when unknown probably. We will replace those occurences with **null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3Nb2oUsdh8O5",
    "outputId": "676d386a-e9fc-42cb-e4ca-d64d4608b59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Null count before cleaning codes: 0\n",
      "- Min value before cleaning codes: -3.0\n",
      "- Max value before cleaning codes: 9999.0\n",
      "- Codes to be replaced with null: [-3.0, 9999.0, 12.5]\n",
      "\n",
      "Verifying replacement:\n",
      "- Null count in ProposedUnitsPerCarton AFTER cleaning codes: 11297\n",
      "  >> Confirmation: Null count increased by 11297 as expected.\n",
      "- Minimum value in ProposedUnitsPerCarton after cleaning: 0.0\n",
      "- Maximum value in ProposedUnitsPerCarton after cleaning: 49.0\n",
      "- Remaining count of codes [-3.0, 9999.0, 12.5]: 0\n",
      "  >> Confirmation: Invalid codes successfully removed.\n",
      "- Remaining count of non-integer values: 0\n",
      "  >> Confirmation: No non-integer values remain.\n"
     ]
    }
   ],
   "source": [
    "# Get original stats BEFORE any code cleaning\n",
    "original_null_count = df_density_report['ProposedUnitsPerCarton'].is_null().sum()\n",
    "original_min = df_density_report['ProposedUnitsPerCarton'].min()\n",
    "original_max = df_density_report['ProposedUnitsPerCarton'].max()\n",
    "print(f\"- Null count before cleaning codes: {original_null_count}\")\n",
    "print(f\"- Min value before cleaning codes: {original_min}\") \n",
    "print(f\"- Max value before cleaning codes: {original_max}\") \n",
    "\n",
    "# Define the list of ALL known invalid code values to replace\n",
    "invalid_codes = [-3.0, 9999.0, 12.5] \n",
    "\n",
    "# Get counts for verification (from previous checks)\n",
    "count_neg_3 = 3754\n",
    "count_9999 = 3786\n",
    "count_12_5 = 3757\n",
    "total_expected_increase = count_neg_3 + count_9999 + count_12_5 \n",
    "\n",
    "print(f\"- Codes to be replaced with null: {invalid_codes}\")\n",
    "\n",
    "\n",
    "# Replace all specific invalid codes \n",
    "df_density_report = df_density_report.with_columns(\n",
    "    pl.when(pl.col('ProposedUnitsPerCarton').is_in(invalid_codes)) \n",
    "    .then(None)                                                  \n",
    "    .otherwise(pl.col('ProposedUnitsPerCarton'))                 \n",
    "    .alias('ProposedUnitsPerCarton')                            \n",
    ")\n",
    "\n",
    "# 1. Check the new null count\n",
    "null_count_after = df_density_report['ProposedUnitsPerCarton'].is_null().sum()\n",
    "print(\"\\nVerifying replacement:\")\n",
    "print(f\"- Null count in ProposedUnitsPerCarton AFTER cleaning codes: {null_count_after}\")\n",
    "if null_count_after == original_null_count + total_expected_increase:\n",
    "    print(f\"  >> Confirmation: Null count increased by {total_expected_increase} as expected.\")\n",
    "else:\n",
    "    print(f\"  >> Warning: Null count change ({null_count_after - original_null_count}) doesn't match expected ({total_expected_increase}). Please review.\")\n",
    "\n",
    "# 2. Check the minimum value (should no longer be negative)\n",
    "min_after_cleaning = df_density_report['ProposedUnitsPerCarton'].min()\n",
    "print(f\"- Minimum value in ProposedUnitsPerCarton after cleaning: {min_after_cleaning}\")\n",
    "\n",
    "# 3. Check the maximum value (should no longer be 9999.0)\n",
    "max_after_cleaning = df_density_report['ProposedUnitsPerCarton'].max()\n",
    "print(f\"- Maximum value in ProposedUnitsPerCarton after cleaning: {max_after_cleaning}\")\n",
    "\n",
    "# 4. Check if the specific codes remain (should be none)\n",
    "remaining_codes = df_density_report.filter(pl.col('ProposedUnitsPerCarton').is_in(invalid_codes)).height\n",
    "print(f\"- Remaining count of codes {invalid_codes}: {remaining_codes}\") \n",
    "if remaining_codes == 0:\n",
    "     print(\"  >> Confirmation: Invalid codes successfully removed.\")\n",
    "else:\n",
    "     print(\"  >> Warning: Some invalid code values still detected.\")\n",
    "\n",
    "# 5. Check if any other decimals remain (should now be 0)\n",
    "remaining_decimals = df_density_report.filter(\n",
    "    (pl.col('ProposedUnitsPerCarton').is_not_null()) &\n",
    "    (pl.col('ProposedUnitsPerCarton') % 1 != 0)\n",
    ").height\n",
    "print(f\"- Remaining count of non-integer values: {remaining_decimals}\") \n",
    "if remaining_decimals == 0:\n",
    "    print(\"  >> Confirmation: No non-integer values remain.\")\n",
    "else:\n",
    "    print(\"  >> Warning: Non-integer values still detected (other than the codes).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBRK-7VWh8O6"
   },
   "source": [
    "Now we have **11,297** null values. We will follow a similar approach to inpute the nulls like we did before, finding the median for each garment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QKGVP2tAh8O7",
    "outputId": "3c6152f3-9e28-4dd6-861b-7613cf3feaff"
   },
   "outputs": [],
   "source": [
    "1. #Calculate median per GarmentType (filter out nulls first)\n",
    "median_map_garment = (\n",
    "    df_density_report.filter(pl.col('ProposedUnitsPerCarton').is_not_null()) # Use only non-null values for calculation\n",
    "    .group_by('GarmentType')\n",
    "    .agg(pl.median('ProposedUnitsPerCarton').alias('MedianUnits_Garment'))\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Calculate global median from valid data\n",
    "global_valid_median = df_density_report.filter(\n",
    "    pl.col('ProposedUnitsPerCarton').is_not_null() # Use only non-null values\n",
    ")['ProposedUnitsPerCarton'].median()\n",
    "\n",
    "\n",
    "# Check if global median calculation was successful\n",
    "if global_valid_median is None:\n",
    "    print(\"Warning: Global median calculation returned None. Please check the data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JPXHeMvQh8O8",
    "outputId": "e33f59dd-e2ff-4542-8303-e4808c52877f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null count in ProposedUnitsPerCarton AFTER imputation: 0\n",
      "Successfully imputed all null values.\n",
      "Describe ProposedUnitsPerCarton after imputation\n",
      "shape: (9, 2)\n",
      "┌────────────┬────────────────────────┐\n",
      "│ statistic  ┆ ProposedUnitsPerCarton │\n",
      "│ ---        ┆ ---                    │\n",
      "│ str        ┆ f64                    │\n",
      "╞════════════╪════════════════════════╡\n",
      "│ count      ┆ 500000.0               │\n",
      "│ null_count ┆ 0.0                    │\n",
      "│ mean       ┆ 24.75393               │\n",
      "│ std        ┆ 11.169403              │\n",
      "│ min        ┆ 0.0                    │\n",
      "│ 25%        ┆ 16.0                   │\n",
      "│ 50%        ┆ 25.0                   │\n",
      "│ 75%        ┆ 32.0                   │\n",
      "│ max        ┆ 49.0                   │\n",
      "└────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 3. Join the calculated group medians back\n",
    "df_impute_step = df_density_report.join(\n",
    "    median_map_garment, on='GarmentType', how='left'\n",
    ")\n",
    "\n",
    "# 4. Impute using coalesce (Original -> Group Median -> Global Median)\n",
    "df_imputed = df_impute_step.with_columns(\n",
    "    pl.coalesce(\n",
    "        pl.col('ProposedUnitsPerCarton'),\n",
    "        pl.col('MedianUnits_Garment'),\n",
    "        pl.lit(global_valid_median)\n",
    "    ).alias('ProposedUnitsPerCarton_Imputed')\n",
    ")\n",
    "\n",
    "# 5. Clean up temporary column and overwrite original\n",
    "df_final = df_imputed.drop(['MedianUnits_Garment'])\n",
    "df_final = df_final.with_columns(\n",
    "    pl.col('ProposedUnitsPerCarton_Imputed').alias('ProposedUnitsPerCarton')\n",
    ").drop('ProposedUnitsPerCarton_Imputed')\n",
    "\n",
    "\n",
    "# Verification\n",
    "final_null_count = df_final['ProposedUnitsPerCarton'].is_null().sum()\n",
    "print(f\"\\nNull count in ProposedUnitsPerCarton AFTER imputation: {final_null_count}\")\n",
    "\n",
    "if final_null_count == 0:\n",
    "    print(\"Successfully imputed all null values.\")\n",
    "    print(\"Describe ProposedUnitsPerCarton after imputation\")\n",
    "    print(df_final.select(pl.col('ProposedUnitsPerCarton')).describe())\n",
    "else:\n",
    "    print(f\"Warning: {final_null_count} null values remain. Review median calculations and fallback.\")\n",
    "\n",
    "df_density_report = df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjEfI6mnUI5_"
   },
   "source": [
    "## 2. Historical Incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1DjgDh3h8PW"
   },
   "source": [
    "There is no need to clean this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4vmAYqoVHzI"
   },
   "source": [
    "## 3. Product Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBaiu_s6Wk7w"
   },
   "source": [
    "There is no need to clean this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNl3APC6WmW9"
   },
   "source": [
    "## 4. Supplier Scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7Ysp2iGih8Pm",
    "outputId": "3649a3c5-4e77-489a-f45d-b844996f53ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing rows where (SupplierName, Month) combinations are duplicated (showing first ~10 duplicates):\n",
      "shape: (10, 8)\n",
      "┌────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ SupplierNa ┆ Month   ┆ PackagesHa ┆ BadPackagi ┆ TotalIncid ┆ AverageCos ┆ OnTimeDel ┆ Anomalies │\n",
      "│ me         ┆ ---     ┆ ndled      ┆ ngRate (%) ┆ ents       ┆ tPerIncide ┆ iveryRate ┆ Detected  │\n",
      "│ ---        ┆ str     ┆ ---        ┆ ---        ┆ ---        ┆ nt (€)     ┆ (%)       ┆ ---       │\n",
      "│ str        ┆         ┆ i64        ┆ f64        ┆ i64        ┆ ---        ┆ ---       ┆ i64       │\n",
      "│            ┆         ┆            ┆            ┆            ┆ f64        ┆ f64       ┆           │\n",
      "╞════════════╪═════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ SupplierA  ┆ 2023-01 ┆ 102        ┆ 19.61      ┆ 1          ┆ 261.0      ┆ 74.56     ┆ 0         │\n",
      "│ SupplierA  ┆ 2023-01 ┆ 76         ┆ 14.47      ┆ 3          ┆ 591.0      ┆ 87.86     ┆ 2         │\n",
      "│ SupplierA  ┆ 2023-01 ┆ 7841       ┆ 8.46       ┆ 133        ┆ 538.23     ┆ 86.01     ┆ 23        │\n",
      "│ SupplierA  ┆ 2023-02 ┆ 7196       ┆ 7.78       ┆ 153        ┆ 572.14     ┆ 88.09     ┆ 16        │\n",
      "│ SupplierA  ┆ 2023-02 ┆ 85         ┆ 14.12      ┆ 6          ┆ 427.67     ┆ 88.06     ┆ 1         │\n",
      "│ SupplierA  ┆ 2023-02 ┆ 75         ┆ 24.0       ┆ 1          ┆ 438.0      ┆ 76.49     ┆ 1         │\n",
      "│ SupplierA  ┆ 2023-03 ┆ 7842       ┆ 7.94       ┆ 163        ┆ 547.51     ┆ 84.74     ┆ 21        │\n",
      "│ SupplierA  ┆ 2023-03 ┆ 103        ┆ 16.5       ┆ 5          ┆ 714.4      ┆ 86.72     ┆ 0         │\n",
      "│ SupplierA  ┆ 2023-03 ┆ 100        ┆ 20.0       ┆ 2          ┆ 250.5      ┆ 83.55     ┆ 1         │\n",
      "│ SupplierA  ┆ 2023-04 ┆ 89         ┆ 20.22      ┆ 3          ┆ 642.42     ┆ 78.75     ┆ 1         │\n",
      "└────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Identify the combinations that are duplicated\n",
    "duplicated_keys = (\n",
    "    df_supplier_scorecard\n",
    "    .group_by(['SupplierName', 'Month'])\n",
    "    .agg(pl.len().alias('count'))\n",
    "    .filter(pl.col('count') > 1)\n",
    "    .select(['SupplierName', 'Month']) # Select only the key columns\n",
    ")\n",
    "\n",
    "# Join back to the original data to get ALL columns for the duplicated keys\n",
    "# Use an inner join to only get rows matching the duplicated keys\n",
    "view_duplicates = duplicated_keys.join(\n",
    "    df_supplier_scorecard,\n",
    "    on=['SupplierName', 'Month'],\n",
    "    how='inner'\n",
    ").sort(['SupplierName', 'Month']) # Sort to see duplicates grouped together\n",
    "\n",
    "print(f\"Viewing rows where (SupplierName, Month) combinations are duplicated (showing first ~10 duplicates):\")\n",
    "\n",
    "# Show enough rows to see a few examples of duplicates\n",
    "print(view_duplicates.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before aggregation: (252, 8)\n",
      "Shape after final aggregation: (144, 8)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Aggregate Data\n",
    "#shape before aggregation\n",
    "print(f\"Shape before aggregation: {df_supplier_scorecard.shape}\")\n",
    "df_scorecard_agg = df_supplier_scorecard.group_by(['SupplierName', 'Month']).agg([\n",
    "    pl.sum('PackagesHandled').alias('PackagesHandled'),\n",
    "    pl.sum('TotalIncidents').alias('TotalIncidents'),\n",
    "    pl.sum('AnomaliesDetected').alias('AnomaliesDetected'),\n",
    "\n",
    "    \n",
    "    (pl.col('PackagesHandled') * pl.col('BadPackagingRate (%)') / 100.0).sum().alias('TotalBadPackages'),\n",
    "    (pl.col('PackagesHandled') * pl.col('OnTimeDeliveryRate (%)') / 100.0).sum().alias('TotalOnTimePackages'),\n",
    "    (pl.col('AverageCostPerIncident (€)') * pl.col('TotalIncidents')).sum().alias('TotalIncidentCost')\n",
    "])\n",
    "\n",
    "# Step 2: Calculate Final Metrics \n",
    "df_scorecard_calculated = df_scorecard_agg.with_columns([\n",
    "    \n",
    "    (pl.when(pl.col('PackagesHandled') > 0)\n",
    "     .then((pl.col('TotalBadPackages') * 100.0) / pl.col('PackagesHandled'))\n",
    "     .otherwise(0.0)\n",
    "    ).alias('BadPackagingRate (%)'),\n",
    "\n",
    "    \n",
    "    (pl.when(pl.col('PackagesHandled') > 0)\n",
    "     .then((pl.col('TotalOnTimePackages') * 100.0) / pl.col('PackagesHandled'))\n",
    "     .otherwise(None)\n",
    "    ).alias('OnTimeDeliveryRate (%)'),\n",
    "\n",
    "    \n",
    "    (pl.when(pl.col('TotalIncidents') > 0)\n",
    "     .then(pl.col('TotalIncidentCost') / pl.col('TotalIncidents'))\n",
    "     .otherwise(0.0)\n",
    "    ).alias('AverageCostPerIncident (€)')\n",
    "])\n",
    "\n",
    "# Step 3: Optional Rounding \n",
    "df_scorecard_rounded = df_scorecard_calculated.with_columns([\n",
    "    pl.col('BadPackagingRate (%)').round(2),\n",
    "    pl.col('OnTimeDeliveryRate (%)').round(2),\n",
    "    pl.col('AverageCostPerIncident (€)').round(2)\n",
    "])\n",
    "\n",
    "# Step 4: Drop Intermediate Columns \n",
    "df_scorecard_final = df_scorecard_rounded.drop([\n",
    "    'TotalBadPackages',\n",
    "    'TotalOnTimePackages',\n",
    "    'TotalIncidentCost'\n",
    "])\n",
    "\n",
    "#shape after final aggregation\n",
    "print(f\"Shape after final aggregation: {df_scorecard_final.shape}\")\n",
    "\n",
    "# Step 6: Assign back\n",
    "df_supplier_scorecard = df_scorecard_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6E6RwcJh8Pp"
   },
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jnJWbLRWX6x-"
   },
   "outputs": [],
   "source": [
    "df_density_report.write_csv(EXPORT_DENSITY_REPORT, separator=\";\")\n",
    "df_historical_incidents.write_csv(EXPORT_HISTORICAL_INCIDENTS, separator=\";\")\n",
    "df_product_attributes.write_csv(EXPORT_PRODUCT_ATTRIBUTES, separator=\";\")\n",
    "df_supplier_scorecard.write_csv(EXPORT_SUPPLIER_SCORECARD, separator=\";\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
